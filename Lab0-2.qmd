---
title: "Lab0"
author: "Eric Vance"
format: pdf
editor: visual
---

## **Lab0:** About Your Team, About Individuals, and Team Collaboration

**Due: 11:59PM Sunday, January 25 on Canvas as a knitted pdf file of a Quarto document**

1.  You will determine your team's name and goals.
2.  You will add to the document your individual sections.
3.  You will practice collaborating on an applied statistical learning "project."

### Instructions for Lab0

1.  Using Quarto, you will complete the first team section "About Team Teamname." See further instructions below.
2.  Each team member will add their own individual section to the team Quarto document. How do you want to collaborate on your document this semester? Github? Google Colab? Something else? See instructions for individual sections below.
3.  As a team, you will complete an applied "project". There will be some individual components (so that everyone gets practice with implementing the stat learning methods) and team components.

### About Your Team

In this team section, include a team photo (all of you), your team name, and your teamâ€™s main goal for this semester for this course (i.e., what does your team want to accomplish by the end of the semester?). Throughout the semester you will be giving feedback to your teammates about how well they are helping your team accomplish its goal(s).

Team Name: ROCstars

Goals: ??

![](images/clipboard-4086253343.png){width="564"}

#### Individual Sections

Each individual must complete their own subsection, which must include:

-   a photo of yourself with a caption explaining the context

    ![](images/clipboard-3686530741.png){width="544"}

This is a photo of me with my mom and 2 sisters this winter break in Puerto Vallarta, MX, taking a sunset selfie before dinner. It was my first time in Mexico and I had a great time.

-   at least one (non-statistics) question you would like to know the answer to that could potentially be answered by applying statistical learning methods to data

I would like to know what kind of music makes me the most productive and maybe on a different line what kind of music gives me the most energy....

-   what you would love to be doing six months after graduation and then five years after that (what would make you excited to be doing?)

I would like to be starting my first full time job by at least six months after graduation. By five years after that I would like to be doing a job that I would really be interested in doing this might be something like a sports analyst.

-   what you hope your greatest career accomplishment will be

I don't have the need or want to be known to have done some crazy achievement but I want to make a good salary and hopefully climb the ranks a little, I would just like to be known for being a good and kind person in any of my jobs while obviously still bringing value and doing something that interests me.

-   and given these hopes and goals, what you are hoping to learn/accomplish/do in this course.

I'm hoping to better my regression skills, my coding, and in general learn things that I can take into a internship. I am excited to learn about Boosting and Random Forests because those sound like interesting regression tools.

-   You must also include something of your own choosing not described above. Anything. Be creative!

Ranking my favorite indoor Rock Climbing activities:

1.  Top Rope
2.  Bouldering
3.  Strength Training, including things like hang boarding, grip strength, and pull ups (If this counts as a category. I think it does though)
4.  Kilter/Moon Board
5.  Lead Climbing (Only because I have never done it but hope to learn)

### Team Applied Section

Find the misclassification rate for K-nearest neighbors (KNN) for a range of k values, given a complicated true generating model. Do this individually. As a team, compare answers and then plot the decision boundary for the team's "best" value of k.

Also, individually practice walking through the steps of Q1, Q2, and Q3 for this "project". Within this section will be a team section (what is the "best" k and what is the decision boundary) and individual sections. In your individual section, describe a plausible story for Q1, Q2, and Q3. Specifically, for Q1: What is Y, X1, and X2, and why should we care?. Make up something plausible that you actually care about. Y could be whether a kitten is adopted from a shelter given X1=age and X2=health of the cat. That's just one of countless plausible examples.

For Q2: Make predictions given X1 and X2 using KNN and k=(1 and 5) or (2 and 6) or (3 and 7) or (4 and 8). That is, each teammate fits KNN for two different values of k. Compute the misclassification rates for your values of k.

For Q3: Using this model and your plausible Q1 scenario, what is the answer for X1=0.5 and X2=0.5? For your scenario, what are some ethical implications of your stat learning modeling?

#### Generating Model

We have a logistic regression generating model. Given $x_1 \in [0,1]$ and $x_2 \in [0,1], Y \sim Ber(p)$, where $p$ is related to $x_1$ and $x_2$ through the logistic link function: $\log(\frac{p}{(1-p)}) = x_1 -2x_2 -2{x_1}^{2} + {x_2}^{2} + 3{x_1}{x_2}$, where $\log$ is the natural log, sometimes written as $\ln$.

The code for this is below.

```{r}
library(class)
library(tidyverse)
library(ggplot2)
```

```{r}
#Generative model
set.seed(116) #setting a random seed so that we can reproduce everything exactly if we want to

generate_y <- function(x1,x2) { #two input parameters to generate the output y
  logit <- x1 -2*x2 -2*x1^2 + x2^2 + 3*x1*x2
  p <- exp(logit)/(1+exp(logit)) #apply the inverse logit function
  y <- rbinom(1,1,p) #y becomes a 0 (with prob 1-p) or a 1 with probability p
}
```

#### Example code

We are going to use our generating model to create a test set of 100 predictors (x1, x2), and then 100 outcomes. Then we plot all three variables just to see what the generating model is doing.

```{r}
# Generate a training dataset with 100 points
set.seed(116)
n = 100
X1 <- runif(n,0,1)
X2 <- runif(n,0,1)

#I'm going to use a for loop to generate 100 y's
Y <- rep(0,n) #initializing my Y to be a vector of 0's
for (i in 1:n) {
  Y[i] <- generate_y(X1[i],X2[i])
}

sum(Y) #How many 0's and 1's were predicted? In this training set, 42% were 1's. However, almost 48% are 1's when n is large. That's quite close to 50/50 so we shouldn't have issues with "imbalance," which is something we'll learn about later in the semester.

training <- cbind(X1,X2,Y) #combining all of my variables into a training dataset
ggplot(data=training, aes(x=X1, y=X2, color=Y)) +
  geom_point()
```

This generating function seems to produce Y's with some spatial pattern in the X1, X2 parameter space, but the regions of 0's and 1's are not very well separated. How well will KNN do to classify/predict Y given new x1 and x2 values?

#### Test dataset

We are going to generate a new set of 100 predictors (x1, x2) and outcomes (y) that we will use as our "ground truth".

So, create the test dataset (using random seed=121) first.

```{r}
# Create the training dataset as above using seed=116
# Create a testing dataset using seed=121

set.seed(121)
n = 100
X1 <- runif(n,0,1)
X2 <- runif(n,0,1)

#I'm going to use a for loop to generate 100 y's
Y <- rep(0,n) #initializing my Y to be a vector of 0's
for (i in 1:n) {
  Y[i] <- generate_y(X1[i],X2[i])
}

sum(Y) #43 1's, which is much closer to the 51.5% true rate
testing <- cbind(X1,X2,Y)

#Let's plot the test set. Does it look like the training set? Yeah, looks similar.
ggplot(data=testing, aes(x=X1, y=X2, color=Y)) +
  geom_point()

predicted.ys1 <- knn(train = training[,1:2], test = testing[,1:2], cl=training[,3],k=1)
predicted.ys <- knn(train = training[,1:2], test = testing[,1:2], cl=training[,3],k=5)
```

#### What individuals need to do

1.  Given the training set (seed=116) and the testing set (seed=121), fit KNN on two different values of k.

    ```{r}

    testing.grid <- expand.grid(X1 = seq(0, 1, by = .01), 
                                X2 = seq(0, 1, by = .01))

    predicted.grid1 <- knn(train = training[,1:2], 
                          test = testing.grid, 
                          cl = training[,3], 
                          k = 1)
    predicted.grid5 <- knn(train = training[,1:2], 
                          test = testing.grid, 
                          cl = training[,3], 
                          k = 5)

    predicted.gridxy <- testing.grid
    predicted.gridxy$Y1 <- as.numeric(predicted.grid1) -1
    predicted.gridxy$Y5 <- as.numeric(predicted.grid5) -1 
    ```

2.  Calculate the misclassification rate for each k. If you don't know how to do this, ask a teammate or the professor.

```{r}
misclass_rate <- function(predictions, truth) {
  1 - sum(predictions == truth) / length(predictions)
}

pred_k1 <- knn(train = training[,1:2],
               test = testing[,1:2],
               cl = training[,3],
               k = 1)

pred_k5 <- knn(train = training[,1:2],
               test = testing[,1:2],
               cl = training[,3],
               k = 5)

mis_k1 <- misclass_rate(pred_k1, testing[,3])
mis_k5 <- misclass_rate(pred_k5, testing[,3])

mis_k1
mis_k5
```

1.  If possible, plot the decision boundaries for your k values.

    ```{r}
    ggplot() +
      # Background: Show the decision regions
      geom_raster(data = predicted.gridxy, aes(x = X1, y = X2, fill = as.factor(Y1)), alpha = 0.3) +
      # The Boundary: The contour line where Y changes
      geom_contour(data = predicted.gridxy, aes(x = X1, y = X2, z = Y1), 
                   breaks = 1.5, color = "black", size = 1) +
      # The actual data points
      geom_point(data = training, aes(x = X1, y = X2, color = as.factor(Y))) +
      scale_fill_manual(values = c("red", "blue"), name = "Region") +
      scale_color_manual(values = c("red", "blue"), name = "Actual Class") +
      labs(title = "KNN Decision Boundary (k=1)",
           subtitle = "Black line indicates the Bayes-style decision boundary") +
       geom_contour(data = predicted.gridxy, aes(x = X1, y = X2, z = Y1 +1),  
                   breaks = 1.5, color = "black", size = 1) +  
    theme_minimal()

    ggplot() +
      # Background: Show the decision regions
      geom_raster(data = predicted.gridxy, aes(x = X1, y = X2, fill = as.factor(Y5)), alpha = 0.3) +
      # The Boundary: The contour line where Y changes
      geom_contour(data = predicted.gridxy, aes(x = X1, y = X2, z = Y5), 
                   breaks = 1.5, color = "black", size = 1) +
      # The actual data points
      geom_point(data = training, aes(x = X1, y = X2, color = as.factor(Y))) +
      scale_fill_manual(values = c("red", "blue"), name = "Region") +
      scale_color_manual(values = c("red", "blue"), name = "Actual Class") +
      labs(title = "KNN Decision Boundary (k=5)",
           subtitle = "Black line indicates the Bayes-style decision boundary") +
       geom_contour(data = predicted.gridxy, aes(x = X1, y = X2, z = Y5 +1),  
                   breaks = 1.5, color = "black", size = 1) +  
    theme_minimal()
    ```

<!-- -->

1.  Summarize the Q1, Q2, and Q3 aspects of this "project." Use your imagination. Everyone should have a different scenario.

Q1. Y can represent whether a soccer shot results in a goal 1=goal, 0=no goal). X1= Shot Distance(0-1 where larger value indicates a farther shot) and X2= Shot Power (0-1 where larger value indicates harder shots). This problem is interesting because teams and analysts often want to know how shot location and shot power influence scoring probability. If we can predict the likelihood of a shot going in, we could evaluate player decision making. An example of this would be if a player shot from 18 yards out (X1=0.18) with a shot power of 50mph (X2=0.5), can we estimate whether it results in a goal.

Q2. To model the relationship between distance, power, and scoring, we applied a K Neares Neighbors classifer. Using the simulated data as training examples, we predicted outcomes for a test set using two values of k, k=1 and k=5. The resulting misclassification rate for k=1 is 0.50 and k=5 is 0.42. For this scenario, increasing k improved predictive accuracy, likely due to smoothing the decision boundary aka. reducing sensitivity to noise. This suggests that performance may benefit from moderate levels of regularization.

Q3. Given X1=0.5 and X2=0.5 the model predicts for both k values 0, which indicates that they would not score. However there are ethical considerations. A model based only on measurable attributes like shot speed and distance ignores contextual variables such as other context like defender pressure or player fatigue. It can also undervalue players strengths.

1.  Think about and discuss with your team some of the bonus questions below.

#### What teams need to do

1.  Find the best k. Plot the decision boundary for that k.
2.  Answer as many of the bonus questions as you can.

#### Bonus questions

Ultimately, we would like to know more about this problem, but we don't necessarily have all the tools yet to answer all of our questions. Here are some bonus questions:

-   What is the misclassification rate for k=1...2j, where j is the "optimal" k. That is, for a whole range of k values?
-   What is the Bayes decision boundary for the optimal k?
-   How is the misclassification rate broken down into variance, bias, and irreducible error of the KNN estimator? Related to these questions are how the misclassification rate changes when we use different training or test sets.
-   What happens to the misclassification rate if we go outside the $[0,1]^2$ parameter space? I.e., if $x_1$ and $x_2$ are $<0$ or $>1$?

**Note: the intended audience for your team document is your teammates, the professor, and your future self.**

**Some intended outcomes from this assignment:**

-   You and your team will learn how to effectively edit a document collaboratively
-   You will think about what you want to accomplish in life and how this course relates to that
-   Your teammates, the professor, and the TA will learn something more about you
-   You will get more experience applying statistical learning methods
-   You will gain experience collaborating with your teammates on an applied problem
-   Get practice evaluating a method, specifically KNN
-   Practice thinking about the whole problem (i.e., Q1Q2Q3, not just the Q2 quantitative parts)
